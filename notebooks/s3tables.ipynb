{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Tablesを試してみるための手順\n",
    "---\n",
    "\n",
    "## 1 Terrafromでインフラ準備\n",
    "\n",
    "### 1.1 Terraformの設定ファイルの準備\n",
    "`env.tfvars.sample` ファイルを参考に `env.tfvars` を作成します。\n",
    "\n",
    "regionに関してはS3Tablesが使えるリージョンを設定します。\n",
    "\n",
    "```toml\n",
    "profile = \"yourprofile\"\n",
    "region  = \"us-east-1\"\n",
    "bucket  = \"yourbucket-in-your-region\"\n",
    "```\n",
    "\n",
    "### 1.2 Terraformの実行\n",
    "DevContainer上から以下のコマンドを実行します。\n",
    "\n",
    "\n",
    "```sh\n",
    "$ make tf-apply\n",
    "```\n",
    "\n",
    "実行が完了した際に設定が出力されます。こちらは後続の手順で利用するのでメモしておきます。\n",
    "\n",
    "![](../images/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 SparkによるS3Tablesの操作\n",
    "SparkからS3Tablesを操作していきます.\n",
    "\n",
    "### 2.1 Spark環境設定\n",
    "変数に環境設定を行なっていきます。\n",
    "\n",
    "---\n",
    "まずは以下のCellでTerraformの出力を参照して、変数の設定をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCOUNT_ID = \"xxxxxxxxxxx\"\n",
    "AWS_REGION = \"us-east-1\"\n",
    "AWS_PROFILE = \"xxxxxxxx\"\n",
    "BUCKET_NAME = \"s3-tables-sample-xxxxxxxxxxx\"\n",
    "NAMESPACE = \"s3_tables_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "その他の設定も変数に入れておきます。ここでpysparkから利用する環境変数も設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "s3tables_bucket_arn = f\"arn:aws:s3tables:{AWS_REGION}:{AWS_ACCOUNT_ID}:bucket/{BUCKET_NAME}\"\n",
    "\n",
    "# pysparkの実行時に読み込まれるAWS接続設定\n",
    "os.environ[\"AWS_PROFILE\"] = AWS_PROFILE\n",
    "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
    "\n",
    "\n",
    "# 以下は任意に変更可能\n",
    "app_name = \"S3TablesSample\"\n",
    "catalog_name = \"s3tablesbucket\"\n",
    "table_name = \"sample_table\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "pysparkの準備を行ないます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Sparkで利用するパッケージを指定します\n",
    "spark_packages = [\n",
    "    # Amazon S3 Tables Catalog for Apache Iceberg\n",
    "    # refs: https://github.com/awslabs/s3-tables-catalog\n",
    "    \"software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.3\",\n",
    "\n",
    "    # Pyspark: 3.5用\n",
    "    # Scala: 2.12用\n",
    "    # refs: https://mvnrepository.com/artifact/org.apache.iceberg/iceberg-spark-runtime-3.5\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1\",\n",
    "\n",
    "    # その他必要になるAWS SDK\n",
    "    \"software.amazon.awssdk:s3:2.20.68\",\n",
    "    \"software.amazon.awssdk:sts:2.20.68\",\n",
    "    \"software.amazon.awssdk:glue:2.20.68\",\n",
    "    \"software.amazon.awssdk:dynamodb:2.20.68\",\n",
    "    \"software.amazon.awssdk:kms:2.20.68\",\n",
    "]\n",
    "\n",
    "\n",
    "# Spark Sessionを生成する関数\n",
    "def get_spark_session(\n",
    "    app_name: str = app_name,\n",
    "    catalog_name: str = catalog_name,\n",
    "    packages: list[str] = spark_packages,\n",
    "    aws_region: str = AWS_REGION,\n",
    "    s3tables_bucket_arn: str = s3tables_bucket_arn,\n",
    ") -> SparkSession:\n",
    "    # SparkSessionの作成\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\n",
    "            \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "            \"com.amazonaws.auth.profile.ProfileCredentialsProvider\",\n",
    "        )\n",
    "        .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "        .config(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "        )\n",
    "        .config(\n",
    "            f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "        )\n",
    "        .config(\n",
    "            f\"spark.sql.catalog.{catalog_name}.catalog-impl\",\n",
    "            \"software.amazon.s3tables.iceberg.S3TablesCatalog\",\n",
    "        )\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.warehouse\", s3tables_bucket_arn)\n",
    "        .config(f\"spark.sql.catalog.{catalog_name}.s3.region\", aws_region)\n",
    "        .config(\"spark.sql.catalog.s3tablesbucket.allow-delete-purge\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 S3 Tablesにテーブル作成と確認\n",
    "ここから[AWSのブログ](https://aws.amazon.com/jp/blogs/news/new-amazon-s3-tables-storage-optimized-for-analytics-workloads/)に合せて作業してみます。\n",
    "\n",
    "\n",
    "---\n",
    "まずは一度SparkSessionを作成します。 この際に必要なパッケージもダウンロードされます。（ある程度時間かかります)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/workspaces/s3-tables-sample/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vscode/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vscode/.ivy2/jars\n",
      "software.amazon.s3tables#s3-tables-catalog-for-iceberg-runtime added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#s3 added as a dependency\n",
      "software.amazon.awssdk#sts added as a dependency\n",
      "software.amazon.awssdk#glue added as a dependency\n",
      "software.amazon.awssdk#dynamodb added as a dependency\n",
      "software.amazon.awssdk#kms added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dd1df947-0bfa-4455-8d2c-cf278a8d8594;1.0\n",
      "\tconfs: [default]\n",
      "\tfound software.amazon.s3tables#s3-tables-catalog-for-iceberg-runtime;0.1.3 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 in central\n",
      "\tfound software.amazon.awssdk#s3;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#aws-xml-protocol;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#aws-query-protocol;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#protocol-core;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#sdk-core;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.68 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#endpoints-spi;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#profiles;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#aws-core;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#regions;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#json-utils;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#third-party-jackson-core;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#auth;2.20.68 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#arns;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#crt-core;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#apache-client;2.20.68 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound software.amazon.awssdk#netty-nio-client;2.20.68 in central\n",
      "\tfound io.netty#netty-codec-http;4.1.86.Final in central\n",
      "\tfound io.netty#netty-common;4.1.86.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.86.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.86.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.86.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.86.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.86.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.86.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.86.Final in central\n",
      "\tfound io.netty#netty-transport-classes-epoll;4.1.86.Final in central\n",
      "\tfound software.amazon.awssdk#sts;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#glue;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#aws-json-protocol;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#dynamodb;2.20.68 in central\n",
      "\tfound software.amazon.awssdk#kms;2.20.68 in central\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/s3tables/s3-tables-catalog-for-iceberg-runtime/0.1.3/s3-tables-catalog-for-iceberg-runtime-0.1.3.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.s3tables#s3-tables-catalog-for-iceberg-runtime;0.1.3!s3-tables-catalog-for-iceberg-runtime.jar (2224ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1!iceberg-spark-runtime-3.5_2.12.jar (2669ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/s3/2.20.68/s3-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#s3;2.20.68!s3.jar (385ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/sts/2.20.68/sts-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#sts;2.20.68!sts.jar (293ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/glue/2.20.68/glue-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#glue;2.20.68!glue.jar (603ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/dynamodb/2.20.68/dynamodb-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#dynamodb;2.20.68!dynamodb.jar (419ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/kms/2.20.68/kms-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#kms;2.20.68!kms.jar (367ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-xml-protocol/2.20.68/aws-xml-protocol-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-xml-protocol;2.20.68!aws-xml-protocol.jar (232ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/protocol-core/2.20.68/protocol-core-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#protocol-core;2.20.68!protocol-core.jar (233ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/arns/2.20.68/arns-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#arns;2.20.68!arns.jar (231ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/profiles/2.20.68/profiles-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#profiles;2.20.68!profiles.jar (232ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/crt-core/2.20.68/crt-core-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#crt-core;2.20.68!crt-core.jar (229ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/sdk-core/2.20.68/sdk-core-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#sdk-core;2.20.68!sdk-core.jar (243ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/auth/2.20.68/auth-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#auth;2.20.68!auth.jar (235ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/http-client-spi/2.20.68/http-client-spi-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#http-client-spi;2.20.68!http-client-spi.jar (242ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/regions/2.20.68/regions-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#regions;2.20.68!regions.jar (243ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/annotations/2.20.68/annotations-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#annotations;2.20.68!annotations.jar (240ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/utils/2.20.68/utils-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#utils;2.20.68!utils.jar (233ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-core/2.20.68/aws-core-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-core;2.20.68!aws-core.jar (234ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/metrics-spi/2.20.68/metrics-spi-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#metrics-spi;2.20.68!metrics-spi.jar (233ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/json-utils/2.20.68/json-utils-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#json-utils;2.20.68!json-utils.jar (237ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/endpoints-spi/2.20.68/endpoints-spi-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#endpoints-spi;2.20.68!endpoints-spi.jar (230ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-query-protocol/2.20.68/aws-query-protocol-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-query-protocol;2.20.68!aws-query-protocol.jar (230ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (232ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (230ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/eventstream/eventstream/1.0.1/eventstream-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.eventstream#eventstream;1.0.1!eventstream.jar (226ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/third-party-jackson-core/2.20.68/third-party-jackson-core-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#third-party-jackson-core;2.20.68!third-party-jackson-core.jar (239ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/apache-client/2.20.68/apache-client-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#apache-client;2.20.68!apache-client.jar (231ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/netty-nio-client/2.20.68/netty-nio-client-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#netty-nio-client;2.20.68!netty-nio-client.jar (242ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.13!httpclient.jar (241ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.13!httpcore.jar (233ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.15!commons-codec.jar (236ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.2!commons-logging.jar (228ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec-http/4.1.86.Final/netty-codec-http-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec-http;4.1.86.Final!netty-codec-http.jar (264ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec-http2/4.1.86.Final/netty-codec-http2-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec-http2;4.1.86.Final!netty-codec-http2.jar (242ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-codec/4.1.86.Final/netty-codec-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-codec;4.1.86.Final!netty-codec.jar (240ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport;4.1.86.Final!netty-transport.jar (236ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-common;4.1.86.Final!netty-common.jar (246ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-buffer/4.1.86.Final/netty-buffer-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-buffer;4.1.86.Final!netty-buffer.jar (240ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-handler/4.1.86.Final/netty-handler-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-handler;4.1.86.Final!netty-handler.jar (242ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport-classes-epoll/4.1.86.Final/netty-transport-classes-epoll-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport-classes-epoll;4.1.86.Final!netty-transport-classes-epoll.jar (237ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-resolver/4.1.86.Final/netty-resolver-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-resolver;4.1.86.Final!netty-resolver.jar (234ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty-transport-native-unix-common/4.1.86.Final/netty-transport-native-unix-common-4.1.86.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty-transport-native-unix-common;4.1.86.Final!netty-transport-native-unix-common.jar (233ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/aws-json-protocol/2.20.68/aws-json-protocol-2.20.68.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#aws-json-protocol;2.20.68!aws-json-protocol.jar (234ms)\n",
      ":: resolution report :: resolve 27432ms :: artifacts dl 15739ms\n",
      "\t:: modules in use:\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-transport-classes-epoll;4.1.86.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.86.Final from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#apache-client;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#arns;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#auth;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-core;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-json-protocol;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-query-protocol;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#aws-xml-protocol;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#crt-core;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#dynamodb;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#endpoints-spi;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#glue;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#json-utils;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#kms;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#netty-nio-client;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#profiles;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#protocol-core;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#regions;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#s3;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sdk-core;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#sts;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#third-party-jackson-core;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.68 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\tsoftware.amazon.s3tables#s3-tables-catalog-for-iceberg-runtime;0.1.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   44  |   44  |   44  |   0   ||   44  |   44  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dd1df947-0bfa-4455-8d2c-cf278a8d8594\n",
      "\tconfs: [default]\n",
      "\t44 artifacts copied, 0 already retrieved (90458kB/58ms)\n",
      "24/12/23 08:32:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Terraformで作成したNamespaceが存在することが確認できます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW NAMESPACES IN `s3tablesbucket`\n",
      "+-------------------+\n",
      "|          namespace|\n",
      "+-------------------+\n",
      "|s3_tables_sample_ns|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()\n",
    "sql = f\"SHOW NAMESPACES IN `{catalog_name}`\"\n",
    "print(sql)\n",
    "spark.sql(sql).show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "現時点ではNamespaceの中にテーブルは存在しません."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES IN `s3tablesbucket`.`s3_tables_sample_ns`\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()\n",
    "sql = f\"SHOW TABLES IN `{catalog_name}`.`{NAMESPACE}`\"\n",
    "print(sql)\n",
    "spark.sql(sql).show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 テーブルを作成\n",
    "---\n",
    "[ブログ](https://aws.amazon.com/jp/blogs/news/new-amazon-s3-tables-storage-optimized-for-analytics-workloads/)の記事に合せてテーブルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS `s3tablesbucket`.`s3_tables_sample_ns`.`sample_table`\n",
      " (id INT,\n",
      "  name STRING,\n",
      "  value INT)\n",
      "  USING iceberg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()\n",
    "sql = f\"\"\"CREATE TABLE IF NOT EXISTS `{catalog_name}`.`{NAMESPACE}`.`{table_name}`\n",
    " (id INT,\n",
    "  name STRING,\n",
    "  value INT)\n",
    "  USING iceberg\n",
    "\"\"\"\n",
    "print(sql)\n",
    "spark.sql(sql)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "テーブルにデータを挿入します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO `s3tablesbucket`.`s3_tables_sample_ns`.`sample_table`\n",
      "  VALUES\n",
      "  (1, 'Jeff', 100),\n",
      "  (2, 'Carmen', 200),\n",
      "  (3, 'Stephen', 300),\n",
      "  (4, 'Andy', 400),\n",
      "  (5, 'Tina', 500),\n",
      "  (6, 'Bianca', 600),\n",
      "  (7, 'Grace', 700)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()\n",
    "sql = f\"\"\"INSERT INTO `{catalog_name}`.`{NAMESPACE}`.`{table_name}`\n",
    "  VALUES\n",
    "  (1, 'Jeff', 100),\n",
    "  (2, 'Carmen', 200),\n",
    "  (3, 'Stephen', 300),\n",
    "  (4, 'Andy', 400),\n",
    "  (5, 'Tina', 500),\n",
    "  (6, 'Bianca', 600),\n",
    "  (7, 'Grace', 700)\n",
    "\"\"\"\n",
    "print(sql)\n",
    "spark.sql(sql)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 テーブルの内容を確認\n",
    "---\n",
    "テーブルが作成されて、SparkからもデータをSELECTできるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW NAMESPACES IN `s3tablesbucket`\n",
      "+-------------------+\n",
      "|          namespace|\n",
      "+-------------------+\n",
      "|s3_tables_sample_ns|\n",
      "+-------------------+\n",
      "\n",
      "SELECT * FROM `s3tablesbucket`.`s3_tables_sample_ns`.`sample_table`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|   name|value|\n",
      "+---+-------+-----+\n",
      "|  1|   Jeff|  100|\n",
      "|  2| Carmen|  200|\n",
      "|  3|Stephen|  300|\n",
      "|  4|   Andy|  400|\n",
      "|  5|   Tina|  500|\n",
      "|  6| Bianca|  600|\n",
      "|  7|  Grace|  700|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()\n",
    "\n",
    "sql = f\"SHOW NAMESPACES IN `{catalog_name}`\"\n",
    "print(sql)\n",
    "spark.sql(sql).show()\n",
    "\n",
    "sql = f\"SELECT * FROM `{catalog_name}`.`{NAMESPACE}`.`{table_name}`\"\n",
    "print(sql)\n",
    "spark.sql(sql).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Athenaからデータを確認\n",
    "\n",
    "### 3.1 Lakeformationの権限付与\n",
    "Athenaからデータを確認するためにはLakeformation経由での権限を付与する必要があります。\n",
    "\n",
    "---\n",
    "以下のboto3関数を実行します。ここでは実行ユーザに対してSELECT権限を割り当てています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import Session\n",
    "session = Session(profile_name=AWS_PROFILE,region_name=AWS_REGION)\n",
    "sts = session.client(\"sts\")\n",
    "lakeformation = session.client(\"lakeformation\")\n",
    "principal=sts.get_caller_identity()[\"Arn\"]\n",
    "lakeformation.grant_permissions(\n",
    "    Principal={\"DataLakePrincipalIdentifier\": principal},\n",
    "    Permissions=[\"SELECT\"],\n",
    "    Resource={\n",
    "        \"Table\": {\n",
    "            \"CatalogId\": f\"s3tablescatalog/{BUCKET_NAME}\",\n",
    "            \"DatabaseName\": NAMESPACE,\n",
    "            \"Name\": table_name,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "AWSコンソールからクエリを実行してみると以下のようにデータが取得できます。\n",
    "\n",
    "画面左のペインに入っている値を確認すると理解が深まります。\n",
    "\n",
    "```sql\n",
    "SELECT * FROM \"sample_table\" limit 10;\n",
    "```\n",
    "\n",
    "![](../images/002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. かたづけ\n",
    "作った環境を片付けていきます。\n",
    "\n",
    "## 4.1 Tableの削除\n",
    "BucketやNamespaceはTableが削除されていないと、削除できません。\n",
    "\n",
    "---\n",
    "まずはS3TablesのTableを削除します。こちらはboto3の関数で実施します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import Session\n",
    "session = Session(profile_name=AWS_PROFILE,region_name=AWS_REGION)\n",
    "s3tables = session.client(\"s3tables\")\n",
    "s3tables.delete_table(\n",
    "    tableBucketARN=s3tables_bucket_arn,\n",
    "    namespace=NAMESPACE,\n",
    "    name=table_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Terraformの削除\n",
    "---\n",
    "DevContainer上から以下のコマンドを実行します。\n",
    "これでTerraformで作成したS3TablesのバケットやNamespaceも削除されます。\n",
    "\n",
    "\n",
    "```sh\n",
    "$ make tf-destroy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
